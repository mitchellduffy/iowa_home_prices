{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an all purpose Mean Absolute Error calculating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(predictors_train, predictors_val, targ_train, targ_val, model, *args, **kwargs):\n",
    "    if args:\n",
    "        model.fit(predictors_train, targ_train, *args)\n",
    "    if kwargs:\n",
    "        model.fit(predictors_train, targ_train, **kwargs)\n",
    "    else:\n",
    "        model.fit(predictors_train, targ_train)\n",
    "    preds_val = model.predict(predictors_val)\n",
    "    mae = mean_absolute_error(targ_val, preds_val)\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Iowa data and define the target (y) vs. rest (X) of the data which could be used for building the model to predict y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_file_path = './data/' # this is the path to the Iowa data that you will use\n",
    "data = pd.read_csv(main_file_path + 'train.csv')\n",
    "data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "# Read the test data\n",
    "test = pd.read_csv(main_file_path + 'test.csv')\n",
    "\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one hot encoded categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_training = pd.get_dummies(X)\n",
    "one_hot_encoded_test = pd.get_dummies(test)\n",
    "final_train, final_test = one_hot_encoded_training.align(one_hot_encoded_test, join='inner', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluding categorical variables in order to use Imputer\n",
    "\n",
    "I'm still a little confused as to why one would want to mark these columns as \"was missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_X(X):\n",
    "    num_X = X.select_dtypes(exclude=['object'])\n",
    "\n",
    "    i_X = num_X.copy()\n",
    "\n",
    "    #cols_with_missing = (col for col in num_X.columns\n",
    "    #                    if num_X[col].isnull().any())\n",
    "    #for col in cols_with_missing:\n",
    "    #    i_X[col + '_was_missing'] = i_X[col].isnull()\n",
    "\n",
    "    my_imputer = Imputer()\n",
    "    i_X = my_imputer.fit_transform(i_X)\n",
    "    return i_X\n",
    "\n",
    "X = impute_X(final_train)\n",
    "# Treat the test data in the same way as training data. In this case, impute\n",
    "test_X = impute_X(final_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is code for Random Forest including computing a guess about optimal number of leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opti_max_leaf = []\n",
    "#for max_leaf_nodes in [5, 25, 50, 75, 100, 125, 150, 200, 250, 500, 1000, 2500, 5000]:\n",
    "#    iowa_model = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "#    my_mae = get_mae(train_X, val_X, train_y, val_y, iowa_model)\n",
    "#    print(\"Max leaf nodes: %d \\t\\t Mean Absolute Error: %d\" %(max_leaf_nodes, my_mae))\n",
    "#    opti_max_leaf.append((max_leaf_nodes, my_mae))\n",
    "\n",
    "#opti_max_leaf = sorted(opti_max_leaf, key=lambda leaf_num: leaf_num[1])\n",
    "\n",
    "#iowa_model = RandomForestRegressor(max_leaf_nodes=opti_max_leaf[0][0], random_state=0)\n",
    "#iowa_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "iowa_model = XGBRegressor(n_estimators=1000, learning_rate=0.08)\n",
    "#silent=True can be done later so you don't have to see so much output\n",
    "#iowa_model.fit(train_X, train_y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15900.888741438355\n"
     ]
    }
   ],
   "source": [
    "print(get_mae(train_X, val_X, train_y, val_y, iowa_model, early_stopping_rounds=8, eval_set=[(val_X, val_y)], verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122183.87 165347.34 182069.83 ... 147391.1  123465.55 241406.47]\n"
     ]
    }
   ],
   "source": [
    "# Use the model to make predictions\n",
    "predicted_prices = iowa_model.predict(test_X)\n",
    "# We will look at the predicted prices to ensure we have something sensible.\n",
    "print(predicted_prices)\n",
    "\n",
    "my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
